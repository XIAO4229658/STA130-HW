{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91408523",
   "metadata": {},
   "source": [
    "1. A) Simple Linear Regression involves a single predictor (independent) variable to explain the variation in a dependent variable. In contrast, Multiple Linear Regression includes two or more predictor variables.\n",
    "Benefits: Multiple Linear Regression can capture more complexity by considering additional factors, improving predictive power and accounting for multivariate influences on the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5665d8",
   "metadata": {},
   "source": [
    "B) A continuous variable can take any numeric value within a range, while an indicator (binary) variable represents categorical data with values like 0 and 1.\n",
    "Linear Forms: With a continuous variable, the model predicts a change in the dependent variable for each unit increase in the predictor. With an indicator variable, the model estimates the difference between the two groups it represents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf56d7",
   "metadata": {},
   "source": [
    "C) Adding an indicator variable alongside a continuous variable in a Multiple Linear Regression model allows the model to adjust for categorical group differences.\n",
    "Linear Forms: Simple Linear Regression with only a continuous variable provides a single slope for all data. In Multiple Linear Regression, the indicator variable creates separate intercepts for each category, capturing group-specific baseline differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d919b",
   "metadata": {},
   "source": [
    "D) Including an interaction term between a continuous and an indicator variable allows the model to adjust the slope of the continuous variable based on group membership, meaning it models different rates of change for each group.\n",
    "Linear Form: The interaction term alters the model by adding a unique slope for each group, rather than a single slope for all, enabling a more tailored fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344ebbe",
   "metadata": {},
   "source": [
    "E) When using only indicator variables derived from a non-binary categorical variable, the model effectively performs a comparison across several groups, using binary encodings for each category.\n",
    "Linear Form: The model becomes a series of binary comparisons, where each dummy variable represents one category, and the intercept represents the reference group. This encoding allows comparisons among groups without implying any ordering or quantitative distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169ab3d",
   "metadata": {},
   "source": [
    "2. Outcome Variable: The outcome (dependent) variable could be sales of sports equipment\n",
    "   Predictor Variables: The primary predictor (independent) variables are TV advertising expenditure and online advertising expenditure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31ffc5",
   "metadata": {},
   "source": [
    "This interaction would allow the model to capture any combined effect of both advertising channels on sales that goes beyond their individual effects.\n",
    "Linear forms without Interaction: In the absence of an interaction term, the linear regression model assumes that TV and online advertising have independent effects on sales.\n",
    "Linear forms with Interaction: Including an interaction term between TV and online advertising captures the combined effect of both mediums on sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c4a57",
   "metadata": {},
   "source": [
    "4. Low R-squared: An R-squared value of 17.6% indicates that the model explains only a small portion of the total variability in the outcome variable. In other words, 82.4% of the variability remains unexplained by this model, suggesting that other factors might be influencing the outcome. This often means that the model's fit to the data is weak overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f35ca",
   "metadata": {},
   "source": [
    "5. A) This cell imports necessary libraries, handles missing data in pokeaman, and splits it into 50% training (pokeaman_train) and 50% testing (pokeaman_test) datasets. This prepares the data for model training and evaluation.\n",
    "B) This cell specifies a simple linear regression model (model3) where HP is predicted using Attack and Defense as predictors.\n",
    "C) This cell predicts HP on the test set using model3, calculates the “in-sample” R-squared (fit on training data) and “out-of-sample” R-squared (on test data), providing insight into the model’s performance and generalization.\n",
    "D) Using many interactions allows the model to capture complex relationships between variables.\n",
    "E) The “in-sample” and “out-of-sample” R-squared values are computed and printed to assess model performance, with a comparison between these metrics indicating how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668cc4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Example data: Let's use a synthetic dataset for illustration\n",
    "# X and y are the predictors and target variable respectively\n",
    "X = np.random.rand(100, 1)  # 100 samples, 1 feature\n",
    "y = 2 * X.squeeze() + 1 + np.random.randn(100)  # Linear relationship with noise\n",
    "\n",
    "# Initialize lists to collect performance metrics\n",
    "in_sample_mse = []\n",
    "out_of_sample_mse = []\n",
    "in_sample_r2 = []\n",
    "out_of_sample_r2 = []\n",
    "\n",
    "# Loop to generate model performance metrics over multiple iterations\n",
    "for i in range(50):  # 50 iterations\n",
    "    # Split data into training and testing (out-of-sample) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # In-sample predictions (on training data)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    # Out-of-sample predictions (on testing data)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics for in-sample and out-of-sample\n",
    "    in_sample_mse.append(mean_squared_error(y_train, y_train_pred))\n",
    "    out_of_sample_mse.append(mean_squared_error(y_test, y_test_pred))\n",
    "    in_sample_r2.append(r2_score(y_train, y_train_pred))\n",
    "    out_of_sample_r2.append(r2_score(y_test, y_test_pred))\n",
    "\n",
    "# Plotting the distributions of the metrics\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# In-sample vs out-of-sample MSE\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(in_sample_mse, bins=15, alpha=0.7, label='In-sample MSE \n",
    "plt.hist(out_of_sample_mse, bins=15, alpha=0.7, label='Out-of-sample MSE')\n",
    "plt.xlabel('Mean Squared Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('MSE Distribution: In-sample vs Out-of-sample')\n",
    "plt.legend()\n",
    "\n",
    "# In-sample vs out-of-sample R^2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(in_sample_r2, bins=15, alpha=0.7, label='In-sample R^2')\n",
    "plt.hist(out_of_sample_r2, bins=15, alpha=0.7, label='Out-of-sample R^2')\n",
    "plt.xlabel('R-squared')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('R^2 Distribution: In-sample vs Out-of-sample')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8192d",
   "metadata": {},
   "source": [
    "8. Meaning of results: By comparing in-sample and out-of-sample results, we can assess if the model is overfitting (performing well on training data but poorly on test data) or underfitting (performing poorly on both).\n",
    "Purpose of demonstration: Evaluate model stability and generalization by comparing performance across multiple iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51ddcd",
   "metadata": {},
   "source": [
    "9. The goal of this code is to assess how well models trained on specific subsets of data generalize to other subsets (e.g., Generation 6). By calculating and comparing in-sample and out-of-sample R-squared values, it provides insights into whether the models are overfitting, underfitting, or generalizing well across different generational data. This analysis helps ensure that the model is robust and not just tailored to specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6edd78",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/67327f6d-1894-8011-9174-fc181fa47254"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
