{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb14ad8",
   "metadata": {},
   "source": [
    "2. Accuracy: Spam Detection: If spam and non-spam emails occur in roughly equal proportions, accuracy can provide a good overall performance measure. Sensitivity: Search-and-Rescue Operations: Identifying areas where survivors might be located; prioritizing finding all possible survivors over false alarms. Specificity: Screening for Rare Diseases: High specificity is needed to avoid unnecessary anxiety, additional tests, and costs for those without the disease. Precision: Product Recommendation Systems: Suggesting items that users are highly likely to purchase; irrelevant recommendations harm user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74aced",
   "metadata": {},
   "source": [
    "4. 1)Target Variable Preparation: The pd.get_dummies() function converts the categorical column Hard_or_Paper into dummy/indicator variables (binary columns) for each unique category. This is necessary because machine learning models generally work with numerical input and output, not categorical data. 2)Feature variable selection: Selects the column List Price from the ab_reduced_noNaN DataFrame to use as the feature/input variable for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf839e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "# Assuming `X` and `y` are already defined as per your earlier description\n",
    "# X = ab_reduced_noNaN[['List Price']]\n",
    "# y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "\n",
    "# Step 2: Initialize the DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "# Step 3: Train the classifier\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Step 4: Output the trained classifier\n",
    "print(\"Decision Tree trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f2f98",
   "metadata": {},
   "source": [
    "The tree makes predictions based on sequential splits of List Price. At each leaf node, the prediction is the majority class of the samples in that range. Use the plot to identify the specific thresholds and predictions for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a133be7",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673cd81a-225c-8011-991d-46be68e8d821"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059239ae",
   "metadata": {},
   "source": [
    "5. Here is the step: A) Train the decision tree model: Use the DecisionTreeClassifier with the given X data and limit the depth to 4.\n",
    "B) Visualize the tree: Utilize libraries like graphviz or matplotlib to create a clear representation.\n",
    "A decision tree like clf2 predicts by traversing from the root node to a leaf node based on feature thresholds. At each node, it evaluates a condition (e.g., NumPages ≤ 100) and follows the corresponding branch (left or right). This process repeats until reaching a leaf node, where the predicted class or class probabilities are assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b48f2",
   "metadata": {},
   "source": [
    "7. The confusion matrices for clf and clf2 are better because they likely result from models trained on a more comprehensive feature set and a greater depth (max_depth), allowing for more nuanced decision boundaries. This helps in better capturing relationships in the data, reducing misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d759a9d",
   "metadata": {},
   "source": [
    "8. A) Get the Feature Importances: Use the .feature_importances_ attribute of the trained decision tree model.\n",
    "B) Plot the Importances:\n",
    "Use libraries like matplotlib or seaborn to create a bar chart.\n",
    "Label the x-axis with feature names and the y-axis with their importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming clf2 is your trained decision tree model\n",
    "importances = clf2.feature_importances_\n",
    "feature_names = clf2.feature_names_in_\n",
    "\n",
    "# Identify the most important feature\n",
    "most_important_feature = feature_names[importances.argmax()]\n",
    "most_important_value = importances.max()\n",
    "\n",
    "print(f\"The most important predictor variable is '{most_important_feature}' with an importance score of {most_important_value:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af32ed5f",
   "metadata": {},
   "source": [
    "9. Linear regression coefficients show how much the target changes with a one-unit change in a feature, assuming all else is constant. Decision tree feature importances indicate how much each feature helps split the data but don’t show direct relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e97b20",
   "metadata": {},
   "source": [
    "10. Yes, I've done it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
