{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c323afb",
   "metadata": {},
   "source": [
    "1. The theoretical Simple Linear Regression model predicts a linear relationship between two variables:  independent X(predictor) and dependent Y(outcome).\n",
    "X: This is the independent variable that we believe influences the outcome. It is a fixed or observed value in each sample.\n",
    "Y: This is the dependent variable, whose value we want to predict based on X.\n",
    "intercept: This is the baseline value of Y when X = 0.\n",
    "slope: The change in X per unit change of Y.\n",
    "In a theoretical regression model, we assume that for any value of X, the distribution of Y is normally distributed around the line defined by intercept plus slope with a variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Parameters for the regression model\n",
    "beta_0 = 5      # Intercept\n",
    "beta_1 = 2      # Slope\n",
    "sigma = 1.5     # Standard deviation of the error term\n",
    "\n",
    "# Generate some predictor values (X) as an array\n",
    "X = np.linspace(0, 10, 100)\n",
    "\n",
    "# Calculate the expected outcome values without error (pure linear relationship)\n",
    "Y_true = beta_0 + beta_1 * X\n",
    "\n",
    "# Simulate the actual outcome values with random error\n",
    "# Each Y is a sample from a normal distribution centered on (beta_0 + beta_1 * X)\n",
    "Y = Y_true + np.random.normal(0, sigma, size=X.shape)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X, Y_true, label=\"True Regression Line (no error)\", color=\"green\", linestyle=\"--\")\n",
    "plt.scatter(X, Y, label=\"Sampled Y values with error\", color=\"blue\", alpha=0.6)\n",
    "\n",
    "# Show normal distributions around selected points on the line to visualize error distribution\n",
    "for x in [2, 5, 8]:\n",
    "    mean_y = beta_0 + beta_1 * x\n",
    "    y_vals = np.linspace(mean_y - 3*sigma, mean_y + 3*sigma, 100)\n",
    "    plt.plot([x]*100, y_vals, 'k-', lw=0.5, alpha=0.5)\n",
    "    plt.fill_betweenx(y_vals, x - 0.3, x + 0.3, color='gray', alpha=0.2)\n",
    "    plt.text(x + 0.1, mean_y, r\"$\\mathcal{N}({}, {})$\".format(round(mean_y, 1), sigma), color=\"black\")\n",
    "\n",
    "plt.xlabel(\"Predictor Variable X\")\n",
    "plt.ylabel(\"Outcome Variable Y\")\n",
    "plt.legend()\n",
    "plt.title(\"Simple Linear Regression Model with Error Term\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b14d71",
   "metadata": {},
   "source": [
    "2. To demonstrate how to fit and visualize a Simple Linear Regression model, we'll first simulate a dataset based on the theoretical model using numpy. Then, we will use pandas to organize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Step 1: Simulate the dataset\n",
    "\n",
    "# Parameters for the regression model\n",
    "beta_0 = 5      # Intercept\n",
    "beta_1 = 2      # Slope\n",
    "sigma = 1.5     # Standard deviation of the error term\n",
    "\n",
    "# Generate some predictor values (X) as an array\n",
    "np.random.seed(42)  # For reproducibility\n",
    "X = np.random.uniform(0, 10, 100)  # Random X values between 0 and 10\n",
    "\n",
    "# Calculate the true outcome values with error term\n",
    "Y = beta_0 + beta_1 * X + np.random.normal(0, sigma, size=X.shape)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'X': X, 'Y': Y})\n",
    "\n",
    "# Step 2: Fit the Simple Linear Regression Model using statsmodels\n",
    "\n",
    "# Define the formula and fit the model\n",
    "model = smf.ols(formula='Y ~ X', data=data).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model.summary())\n",
    "\n",
    "# Step 3: Visualize the fitted regression line and the data points\n",
    "\n",
    "# Scatter plot of the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['X'], data['Y'], color='blue', label='Data points', alpha=0.6)\n",
    "\n",
    "# Plot the fitted regression line\n",
    "plt.plot(data['X'], model.fittedvalues, color='red', label='Fitted Regression Line', linewidth=2)\n",
    "\n",
    "# Label the plot\n",
    "plt.xlabel(\"Predictor Variable X\")\n",
    "plt.ylabel(\"Outcome Variable Y\")\n",
    "plt.legend()\n",
    "plt.title(\"Fitted Simple Linear Regression Model\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6e802",
   "metadata": {},
   "source": [
    "3. To add the line from Question 1 (the theoretical regression line) to the fitted regression line in the figure from Question 2, we’ll modify the plotting code to include both lines for comparison. The theoretical line represents the \"true\" model used to generate the data, while the fitted line represents the model estimated from the sample data.\n",
    "Difference explanation:\n",
    "Theoretical line: It represents the underlying data-generating process, this line is fixed and does not vary based on the sample.\n",
    "Fitted line: The fitted line is the regression line calculated from our sample data. It is our estimate of the true relationship based on this specific sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7126ef0",
   "metadata": {},
   "source": [
    "4. Each value in fitted_model.fittedvalues is computed similarly for all observed X values in the dataset, using these estimated coefficients from fitted_model.params. This process provides us with a fitted line that represents the best linear approximation of the relationship between X and Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee980335",
   "metadata": {},
   "source": [
    "5. The Ordinary Least Squares (OLS) method chooses the line that minimizes the sum of squared differences between observed and predicted Y values. Squaring the differences ensures all deviations are positive and gives more weight to larger errors, allowing OLS to find the line that best fits the data by minimizing overall prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b08f2",
   "metadata": {},
   "source": [
    "6. In Simple Linear Regression, R square, shows the proportion of variation in Y explained by the model. It’s calculated by comparing the variation in Y. Higher R square means a better fit.\n",
    "The expression mentioned captures the same idea, showing R square as the square of the correlation between Y and the model's predictions, indicating how well X predicts Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31612db",
   "metadata": {},
   "source": [
    "7. Linearity: The model assumes a linear relationship between X and Y, meaning the change in X leads to the proportion change in Y. If the data shows a curved or non-linear pattern, the linearity assumption is violated, and a simple linear model may not be appropriate.\n",
    "Homoscedasticity: If the residuals spread out more at higher or lower values of X, this assumption of homoscedasticity is violated, suggesting that the model's accuracy varies across different levels of X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0640d",
   "metadata": {},
   "source": [
    " 8. p < 0.05, moderate evidence against the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7832d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load the Old Faithful dataset\n",
    "old_faithful = sns.load_dataset('geyser')\n",
    "\n",
    "# Subset the data for long wait times (waiting > 63)\n",
    "long_wait = old_faithful[old_faithful['waiting'] > 63]\n",
    "\n",
    "# 1. Bootstrap Sampling and Visualizing Slope Coefficients\n",
    "n_bootstrap = 1000  # Number of bootstrap samples\n",
    "slope_bootstrap = []\n",
    "\n",
    "# Generate bootstrap samples, fit models, and collect slope coefficients\n",
    "for _ in range(n_bootstrap):\n",
    "    bootstrap_sample = resample(long_wait)\n",
    "    model = smf.ols('duration ~ waiting', data=bootstrap_sample).fit()\n",
    "    slope_bootstrap.append(model.params['waiting'])\n",
    "\n",
    "# Visualize the distribution of bootstrapped slope coefficients\n",
    "sns.histplot(slope_bootstrap, kde=True, bins=30, color='blue')\n",
    "plt.title(\"Bootstrap Sampling Distribution of the Slope Coefficients\")\n",
    "plt.xlabel('Slope Coefficients')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 2. Simulate Samples Under the Null Hypothesis\n",
    "n_simulations = 1000  # Number of simulations\n",
    "slope_simulated = []\n",
    "\n",
    "# Generate simulated data under the null hypothesis (intercept = 1.65, slope = 0, stddev = 0.37)\n",
    "for _ in range(n_simulations):\n",
    "    # Simulate waiting times based on observed waiting times\n",
    "    waiting_simulated = np.random.choice(long_wait['waiting'], size=160, replace=True)\n",
    "    # Simulate durations using the null model\n",
    "    error_term = np.random.normal(0, 0.37, size=160)  # Standard deviation = 0.37\n",
    "    duration_simulated = 1.65 + 0 * waiting_simulated + error_term  # Slope = 0 under null hypothesis\n",
    "    \n",
    "    # Fit the model and collect the slope coefficient\n",
    "    model = smf.ols('duration ~ waiting', data=pd.DataFrame({'waiting': waiting_simulated, 'duration': duration_simulated})).fit()\n",
    "    slope_simulated.append(model.params['waiting'])\n",
    "\n",
    "# Visualize the bootstrapped sampling distribution under the null hypothesis\n",
    "sns.histplot(slope_simulated, kde=True, bins=30, color='red')\n",
    "plt.title(\"Simulated Sampling Distribution of the Slope Coefficients (Null Hypothesis)\")\n",
    "plt.xlabel('Slope Coefficients')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 3. Report Confidence Interval and p-value\n",
    "\n",
    "# 95% Confidence Interval from the bootstrap distribution\n",
    "ci_lower = np.percentile(slope_bootstrap, 2.5)\n",
    "ci_upper = np.percentile(slope_bootstrap, 97.5)\n",
    "print(f\"95% Confidence Interval for the Slope Coefficient: ({ci_lower:.4f}, {ci_upper:.4f})\")\n",
    "\n",
    "# Check if 0 is in the 95% Confidence Interval\n",
    "contains_zero = 0 >= ci_lower and 0 <= ci_upper\n",
    "print(f\"Does the 95% CI contain 0? {contains_zero}\")\n",
    "\n",
    "# Compare p-value from the fitted model to the simulated model's p-value\n",
    "model_full = smf.ols('duration ~ waiting', data=long_wait).fit()\n",
    "p_value_full = model_full.pvalues['waiting']\n",
    "print(f\"P-value from the full model fit: {p_value_full:.4f}\")\n",
    "\n",
    "# Simulated p-value calculation\n",
    "# Calculate the proportion of simulated slopes greater than or equal to the full model's slope\n",
    "simulated_p_value = np.mean(np.array(slope_simulated) >= model_full.params['waiting'])\n",
    "print(f\"Simulated p-value: {simulated_p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf33e2",
   "metadata": {},
   "source": [
    "12. Model 1 (All Data using slope) is most likely to support the assumption of normality of residuals if its histogram looks symmetric, bell-shaped, and aligns closely with the normal distribution curve.\n",
    "Models 2, 3, and 4 might fail to support the normality assumption due to their specific data subsets and model structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db00623",
   "metadata": {},
   "source": [
    "13. A) Steps for the permutation test:\n",
    "Null hypothesis: There is no difference in duration between the short and long wait groups. This means that the labels of the two groups (short and long) are exchangeable.\n",
    "Test Statistic: The observed difference in means between the short and long wait times.\n",
    "Shuffling Process: Randomly shuffle the waiting labels (without regard to the actual group labels) and calculate the new difference in means. Repeat this process many times (e.g., 10,000 times) to build a distribution of differences under the null hypothesis.\n",
    "p-value: The proportion of permuted differences that are as extreme (or more extreme) than the observed difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65529ae4",
   "metadata": {},
   "source": [
    "B) Steps for Bootstrapping:\n",
    "Sample with Replacement: For each group (short and long wait), sample with replacement and calculate the mean of the bootstrap sample.\n",
    "Repeat: Perform this resampling process a large number of times.\n",
    "Compute the Difference: For each bootstrap sample, compute the difference in the means between the two groups.\n",
    "Percentile Method: Calculate the 2.5th and 97.5th percentiles of the bootstrapped differences to form the 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff4f3d",
   "metadata": {},
   "source": [
    "Permutation Test: The p-value helps us determine whether the difference in means is statistically significant by comparing it to a threshold (typically 0.05).\n",
    "Bootstrap Confidence Interval: The 95% confidence interval provides a range of plausible values for the difference in means, and it can be used to assess the statistical significance of the difference.\n",
    "By applying both methods, you can assess the strength of evidence for or against a difference in the eruption durations between short and long wait times in the Old Faithful dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8fa57",
   "metadata": {},
   "source": [
    "14. I've read through the wiki textbook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
